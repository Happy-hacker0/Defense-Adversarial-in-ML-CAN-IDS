{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxwang\\anaconda3\\envs\\Adversarial_attack_\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten,Conv1D,MaxPool1D\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump, load\n",
    "import queue\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os, sys\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset():\n",
    "        with open('./Dataset/csv/Original/Attack_merge.csv', newline='') as csvfile:\n",
    "                rows = pd.read_csv(csvfile,header=None)\n",
    "                y = rows[10]\n",
    "                x = rows.drop([10], axis=1)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(x, y , test_size=0.2)\n",
    "        \n",
    "        if not os.path.exists(\"./models\"):\n",
    "                os.mkdir(\"./models\")\n",
    "        return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = import_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704672479320366\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97     22845\n",
      "           1       0.98      0.96      0.97     21885\n",
      "\n",
      "    accuracy                           0.97     44730\n",
      "   macro avg       0.97      0.97      0.97     44730\n",
      "weighted avg       0.97      0.97      0.97     44730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(np.array(X_train), np.array(y_train))\n",
    "print(model.score(np.array(X_test), np.array(y_test)))\n",
    "dump(model, './models/DT.joblib')\n",
    "prediction = model.predict(np.array(X_test))\n",
    "print(classification_report(np.array(y_test),prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696496266513498\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "pred_labels = np.rint(preds)\n",
    "f1 = sklearn.metrics.f1_score(y_test, pred_labels, average=\"binary\")\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9659959758551308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97     22845\n",
      "           1       0.97      0.96      0.97     21885\n",
      "\n",
      "    accuracy                           0.97     44730\n",
      "   macro avg       0.97      0.97      0.97     44730\n",
      "weighted avg       0.97      0.97      0.97     44730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=20, max_depth=4)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "dump(model, './models/RF.joblib')\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9069081153588195\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.91     22845\n",
      "           1       0.90      0.91      0.91     21885\n",
      "\n",
      "    accuracy                           0.91     44730\n",
      "   macro avg       0.91      0.91      0.91     44730\n",
      "weighted avg       0.91      0.91      0.91     44730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=3)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "dump(model, './models/LR.joblib')\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxwang\\anaconda3\\envs\\Adversarial_attack_\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:13:32] WARNING: D:\\bld\\xgboost-split_1645118015404\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.9774647887323944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     22845\n",
      "           1       0.97      0.99      0.98     21885\n",
      "\n",
      "    accuracy                           0.98     44730\n",
      "   macro avg       0.98      0.98      0.98     44730\n",
      "weighted avg       0.98      0.98      0.98     44730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = XGBClassifier(booster=\"gbtree\", min_split_loss=7.100747859845302e-07, alpha=0.0010277375135306342, \n",
    "                        max_depth=9, eta=0.6737504946980999, gamma=6.101266632438708e-07, grow_policy=\"lossguide\")\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "dump(model, './models/XGB.joblib')\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864989939637827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85     22845\n",
      "           1       0.78      1.00      0.88     21885\n",
      "\n",
      "    accuracy                           0.86     44730\n",
      "   macro avg       0.89      0.87      0.86     44730\n",
      "weighted avg       0.89      0.86      0.86     44730\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxwang\\anaconda3\\envs\\Adversarial_attack_\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(C = 2, verbose=False)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "dump(model, './models/SVM.joblib')\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9751397272524033\n",
      "[0 0 1 ... 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98     22845\n",
      "           1       0.97      0.98      0.97     21885\n",
      "\n",
      "    accuracy                           0.98     44730\n",
      "   macro avg       0.98      0.98      0.98     44730\n",
      "weighted avg       0.98      0.98      0.98     44730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))\n",
    "dump(model, './models/KNN.joblib')\n",
    "prediction = model.predict(X_test)\n",
    "print(prediction)\n",
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/10\n",
      "4194/4194 [==============================] - 28s 7ms/step - loss: 0.2202 - accuracy: 0.9033 - val_loss: 0.1739 - val_accuracy: 0.9148\n",
      "Epoch 2/10\n",
      "4194/4194 [==============================] - 30s 7ms/step - loss: 0.1924 - accuracy: 0.9117 - val_loss: 0.1729 - val_accuracy: 0.9233\n",
      "Epoch 3/10\n",
      "4194/4194 [==============================] - 29s 7ms/step - loss: 0.1858 - accuracy: 0.9137 - val_loss: 0.1655 - val_accuracy: 0.9271\n",
      "Epoch 4/10\n",
      "4194/4194 [==============================] - 30s 7ms/step - loss: 0.1808 - accuracy: 0.9171 - val_loss: 0.1627 - val_accuracy: 0.9260\n",
      "Epoch 5/10\n",
      "4194/4194 [==============================] - 29s 7ms/step - loss: 0.1773 - accuracy: 0.9181 - val_loss: 0.1660 - val_accuracy: 0.9272\n",
      "Epoch 6/10\n",
      "4194/4194 [==============================] - 30s 7ms/step - loss: 0.1743 - accuracy: 0.9196 - val_loss: 0.1614 - val_accuracy: 0.9272\n",
      "Epoch 7/10\n",
      "4194/4194 [==============================] - 30s 7ms/step - loss: 0.1745 - accuracy: 0.9198 - val_loss: 0.1623 - val_accuracy: 0.9270\n",
      "Epoch 8/10\n",
      "4194/4194 [==============================] - 30s 7ms/step - loss: 0.1736 - accuracy: 0.9197 - val_loss: 0.1654 - val_accuracy: 0.9173\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/10\n",
      "4194/4194 [==============================] - 30s 7ms/step - loss: 0.1651 - accuracy: 0.9233 - val_loss: 0.1537 - val_accuracy: 0.9296\n",
      "Epoch 10/10\n",
      "4194/4194 [==============================] - 30s 7ms/step - loss: 0.1640 - accuracy: 0.9238 - val_loss: 0.1531 - val_accuracy: 0.9296\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('./models/DNN4.h5', save_weights_only=False, save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "dnn4 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(10, activation = 'relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(8, activation = 'relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(4, activation = 'relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.01),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "dnn4.compile(optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']);\n",
    "\n",
    "dnn4.fit(X_train, y_train, batch_size=batch_size, epochs=10, verbose=1, callbacks=[earlyStopping, mcp_save, reduce_lr_loss], validation_split=0.25)\n",
    "model = tf.keras.models.load_model('./models/DNN4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/DNN4.h5\")\n",
    "model = tf.keras.models.load_model('./models/DNN4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.920947909680304\n",
      "f1: 0.9184539458512062\n",
      "prec: 0.9271779112538995\n",
      "recall: 0.9098926205163353\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "pred_labels = np.rint(preds)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels[0:,1])\n",
    "f1 = sklearn.metrics.f1_score(y_test, pred_labels[0:,1], average=\"binary\")\n",
    "prec = sklearn.metrics.precision_score(y_test, pred_labels[0:,1], average=\"binary\")\n",
    "recall = sklearn.metrics.recall_score(y_test, pred_labels[0:,1], average=\"binary\")\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "print(\"f1: \" + str(f1))\n",
    "print(\"prec: \" + str(prec))\n",
    "print(\"recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import newaxis\n",
    "y_train_ex = y_train[:, newaxis]\n",
    "X_train_ex = X_train[:, :, newaxis]\n",
    "y_test_ex = y_test[:, newaxis]\n",
    "X_test_ex = X_test[:, :, newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 10, 100)           600       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 10, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 10, 200)           100200    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 10, 200)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 10, 400)           800400    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 400)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 2001      \n",
      "=================================================================\n",
      "Total params: 903,201\n",
      "Trainable params: 903,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "699/699 [==============================] - 9s 8ms/step - loss: 0.4606 - accuracy: 0.8844\n",
      "Epoch 2/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1724 - accuracy: 0.9233\n",
      "Epoch 3/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1647 - accuracy: 0.9255\n",
      "Epoch 4/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1620 - accuracy: 0.9264\n",
      "Epoch 5/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1599 - accuracy: 0.9268\n",
      "Epoch 6/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1590 - accuracy: 0.9272\n",
      "Epoch 7/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1580 - accuracy: 0.9277\n",
      "Epoch 8/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1570 - accuracy: 0.9280\n",
      "Epoch 9/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1565 - accuracy: 0.9284\n",
      "Epoch 10/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1551 - accuracy: 0.9290\n",
      "Epoch 11/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1543 - accuracy: 0.9292\n",
      "Epoch 12/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1542 - accuracy: 0.9291\n",
      "Epoch 13/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1531 - accuracy: 0.9295\n",
      "Epoch 14/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1531 - accuracy: 0.9295\n",
      "Epoch 15/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1533 - accuracy: 0.9293\n",
      "Epoch 16/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1524 - accuracy: 0.9297\n",
      "Epoch 17/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1526 - accuracy: 0.9294\n",
      "Epoch 18/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1520 - accuracy: 0.9298\n",
      "Epoch 19/100\n",
      "699/699 [==============================] - 6s 8ms/step - loss: 0.1523 - accuracy: 0.9298\n",
      "Epoch 20/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1528 - accuracy: 0.9294\n",
      "Epoch 21/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1518 - accuracy: 0.9298\n",
      "Epoch 22/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1519 - accuracy: 0.9297\n",
      "Epoch 23/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1523 - accuracy: 0.9292\n",
      "Epoch 24/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1553 - accuracy: 0.9282\n",
      "Epoch 25/100\n",
      "699/699 [==============================] - 6s 8ms/step - loss: 0.1534 - accuracy: 0.9289\n",
      "Epoch 26/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1528 - accuracy: 0.9289\n",
      "Epoch 27/100\n",
      "699/699 [==============================] - 6s 8ms/step - loss: 0.1513 - accuracy: 0.9296\n",
      "Epoch 28/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1528 - accuracy: 0.9288\n",
      "Epoch 29/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1513 - accuracy: 0.9297\n",
      "Epoch 30/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1532 - accuracy: 0.9285\n",
      "Epoch 31/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1519 - accuracy: 0.9289\n",
      "Epoch 32/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1529 - accuracy: 0.9289\n",
      "Epoch 33/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1526 - accuracy: 0.9288\n",
      "Epoch 34/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1500 - accuracy: 0.9300\n",
      "Epoch 35/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1517 - accuracy: 0.9291\n",
      "Epoch 36/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1532 - accuracy: 0.9287\n",
      "Epoch 37/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1533 - accuracy: 0.9284\n",
      "Epoch 38/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1518 - accuracy: 0.9290\n",
      "Epoch 39/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1515 - accuracy: 0.9289\n",
      "Epoch 40/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1529 - accuracy: 0.9287\n",
      "Epoch 41/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1515 - accuracy: 0.9291\n",
      "Epoch 42/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1535 - accuracy: 0.9284\n",
      "Epoch 43/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1511 - accuracy: 0.9293\n",
      "Epoch 44/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1518 - accuracy: 0.9291\n",
      "Epoch 45/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1530 - accuracy: 0.9284\n",
      "Epoch 46/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1530 - accuracy: 0.9284\n",
      "Epoch 47/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1520 - accuracy: 0.9291\n",
      "Epoch 48/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1496 - accuracy: 0.9299\n",
      "Epoch 49/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1501 - accuracy: 0.9295\n",
      "Epoch 50/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1505 - accuracy: 0.9295\n",
      "Epoch 51/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1495 - accuracy: 0.9299\n",
      "Epoch 52/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1502 - accuracy: 0.9296\n",
      "Epoch 53/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1517 - accuracy: 0.9293\n",
      "Epoch 54/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1518 - accuracy: 0.9295\n",
      "Epoch 55/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1501 - accuracy: 0.9299\n",
      "Epoch 56/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1500 - accuracy: 0.9294\n",
      "Epoch 57/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1499 - accuracy: 0.9298\n",
      "Epoch 58/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1487 - accuracy: 0.9302\n",
      "Epoch 59/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1512 - accuracy: 0.9291\n",
      "Epoch 60/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1532 - accuracy: 0.9285\n",
      "Epoch 61/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1540 - accuracy: 0.9280\n",
      "Epoch 62/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1530 - accuracy: 0.9284\n",
      "Epoch 63/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1524 - accuracy: 0.9290\n",
      "Epoch 64/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1531 - accuracy: 0.9285\n",
      "Epoch 65/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1524 - accuracy: 0.9285\n",
      "Epoch 66/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1534 - accuracy: 0.9283\n",
      "Epoch 67/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1530 - accuracy: 0.9285\n",
      "Epoch 68/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1535 - accuracy: 0.9282\n",
      "Epoch 69/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1527 - accuracy: 0.9287\n",
      "Epoch 70/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1528 - accuracy: 0.9288\n",
      "Epoch 71/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1538 - accuracy: 0.9281\n",
      "Epoch 72/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1545 - accuracy: 0.9278\n",
      "Epoch 73/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1559 - accuracy: 0.9267\n",
      "Epoch 74/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1545 - accuracy: 0.9275\n",
      "Epoch 75/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1529 - accuracy: 0.9283\n",
      "Epoch 76/100\n",
      "699/699 [==============================] - 5s 8ms/step - loss: 0.1529 - accuracy: 0.9286\n",
      "Epoch 77/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1528 - accuracy: 0.9285\n",
      "Epoch 78/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1542 - accuracy: 0.9277\n",
      "Epoch 79/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1543 - accuracy: 0.9277\n",
      "Epoch 80/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1527 - accuracy: 0.9290\n",
      "Epoch 81/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1530 - accuracy: 0.9281\n",
      "Epoch 82/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1534 - accuracy: 0.9281\n",
      "Epoch 83/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1535 - accuracy: 0.9283\n",
      "Epoch 84/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1536 - accuracy: 0.9279\n",
      "Epoch 85/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1553 - accuracy: 0.9271\n",
      "Epoch 86/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1543 - accuracy: 0.9274\n",
      "Epoch 87/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1525 - accuracy: 0.9285\n",
      "Epoch 88/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1532 - accuracy: 0.9283\n",
      "Epoch 89/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1532 - accuracy: 0.9282\n",
      "Epoch 90/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1537 - accuracy: 0.9279\n",
      "Epoch 91/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1544 - accuracy: 0.9274\n",
      "Epoch 92/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1539 - accuracy: 0.9279\n",
      "Epoch 93/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1526 - accuracy: 0.9281\n",
      "Epoch 94/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1523 - accuracy: 0.9286\n",
      "Epoch 95/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1519 - accuracy: 0.9285\n",
      "Epoch 96/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1535 - accuracy: 0.9280\n",
      "Epoch 97/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1530 - accuracy: 0.9281\n",
      "Epoch 98/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1530 - accuracy: 0.9281\n",
      "Epoch 99/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1517 - accuracy: 0.9287\n",
      "Epoch 100/100\n",
      "699/699 [==============================] - 5s 7ms/step - loss: 0.1537 - accuracy: 0.9278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21e859ddc70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding, GlobalAveragePooling1D\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=100, kernel_size=5, padding='same',activation='relu',strides=1, input_shape=(X_train_ex.shape[1],1))) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=200, kernel_size=5, padding='same', activation='relu',strides=1))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=400, kernel_size=10, padding='same', activation='relu',strides=1))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(X_train_ex, y_train, epochs=epochs, batch_size=batch_size,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"./models/CNN.h5\")\n",
    "model = tf.keras.models.load_model(\"./models/CNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9307623518891125\n",
      "f1: 0.927847540945414\n",
      "prec: 0.9465253351079\n",
      "recall: 0.9098926205163353\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test_ex)\n",
    "pred_labels = np.rint(preds)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test_ex, pred_labels)\n",
    "f1 = sklearn.metrics.f1_score(y_test_ex, pred_labels, average=\"binary\")\n",
    "prec = sklearn.metrics.precision_score(y_test_ex, pred_labels, average=\"binary\")\n",
    "recall = sklearn.metrics.recall_score(y_test_ex, pred_labels, average=\"binary\")\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "print(\"f1: \" + str(f1))\n",
    "print(\"prec: \" + str(prec))\n",
    "print(\"recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 50)            10400     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 71,051\n",
      "Trainable params: 71,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train_ex.shape[1], X_train_ex.shape[2])))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(Dense(1, activation='sigmoid'))\n",
    "regressor.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n",
    "regressor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5592/5592 [==============================] - 62s 10ms/step - loss: 0.1800 - accuracy: 0.9172\n",
      "Epoch 2/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1623 - accuracy: 0.9261\n",
      "Epoch 3/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1596 - accuracy: 0.9266\n",
      "Epoch 4/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1606 - accuracy: 0.9256\n",
      "Epoch 5/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1566 - accuracy: 0.9276\n",
      "Epoch 6/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1547 - accuracy: 0.9279\n",
      "Epoch 7/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1539 - accuracy: 0.9281\n",
      "Epoch 8/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1532 - accuracy: 0.9286\n",
      "Epoch 9/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1543 - accuracy: 0.9279\n",
      "Epoch 10/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1506 - accuracy: 0.9296\n",
      "Epoch 11/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1508 - accuracy: 0.9297\n",
      "Epoch 12/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1499 - accuracy: 0.9298\n",
      "Epoch 13/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1522 - accuracy: 0.9290\n",
      "Epoch 14/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1522 - accuracy: 0.9283\n",
      "Epoch 15/20\n",
      "5592/5592 [==============================] - 58s 10ms/step - loss: 0.1183 - accuracy: 0.9425\n",
      "Epoch 16/20\n",
      "5592/5592 [==============================] - 57s 10ms/step - loss: 0.0914 - accuracy: 0.9567\n",
      "Epoch 17/20\n",
      "5592/5592 [==============================] - 56s 10ms/step - loss: 0.0876 - accuracy: 0.9601\n",
      "Epoch 18/20\n",
      "5592/5592 [==============================] - 57s 10ms/step - loss: 0.0864 - accuracy: 0.9613\n",
      "Epoch 19/20\n",
      "5592/5592 [==============================] - 56s 10ms/step - loss: 0.0873 - accuracy: 0.9616\n",
      "Epoch 20/20\n",
      "5592/5592 [==============================] - 57s 10ms/step - loss: 0.0828 - accuracy: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x221ad599d90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 進行訓練\n",
    "regressor.fit(X_train_ex, y_train_ex, epochs = 20, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.save(\"./models/LSTM.h5\")\n",
    "model = tf.keras.models.load_model(\"./models/LSTM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9685222445785826\n",
      "f1: 0.9678509452918074\n",
      "prec: 0.9672767103281457\n",
      "recall: 0.9684258624628741\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test_ex)\n",
    "pred_labels = np.rint(preds)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test_ex, pred_labels)\n",
    "f1 = sklearn.metrics.f1_score(y_test_ex, pred_labels, average=\"binary\")\n",
    "prec = sklearn.metrics.precision_score(y_test_ex, pred_labels, average=\"binary\")\n",
    "recall = sklearn.metrics.recall_score(y_test_ex, pred_labels, average=\"binary\")\n",
    "print(\"accuracy: \" + str(accuracy))\n",
    "print(\"f1: \" + str(f1))\n",
    "print(\"prec: \" + str(prec))\n",
    "print(\"recall: \" + str(recall))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Adversarial_attack_')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dab91cb55a779ebb65e73a37577434c9648b8bfd406d9f159bac9d9fb993402"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
